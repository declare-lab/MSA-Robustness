nohup: ignoring input
mosei
mosei
mosei
Configurations
{'activation': <class 'torch.nn.modules.activation.ReLU'>,
 'batch_size': 16,
 'clip': 1.0,
 'data': 'mosei',
 'data_dir': PosixPath('/data/yingting/datasets/MOSEI'),
 'dataset_dir': PosixPath('/data/yingting/datasets/MOSEI'),
 'diff_weight': 0.3,
 'dropout': 0.5,
 'embedding_size': 300,
 'eval_batch_size': 10,
 'hidden_size': 128,
 'is_test': False,
 'learning_rate': 4e-05,
 'mode': 'train',
 'model': 'MISA',
 'n_epoch': 500,
 'name': '2022-01-15_19:15:40',
 'num_classes': 1,
 'optimizer': <class 'torch.optim.adam.Adam'>,
 'patience': 6,
 'recon_weight': 1.0,
 'reverse_grad_weight': 1.0,
 'rnncell': 'lstm',
 'runs': 5,
 'sdk_dir': PosixPath('/home/yingting/MISA/CMU-MultimodalSDK'),
 'sim_weight': 1.0,
 'sp_weight': 0.0,
 'test_changed_modal': 'language',
 'test_changed_pct': 0.3,
 'test_method': 'missing',
 'train_changed_modal': 'language',
 'train_changed_pct': 0.0,
 'train_method': 'missing',
 'use_bert': True,
 'use_cmd_sim': True,
 'word_emb_path': '/data/yingting/Glove/glove.840B.300d.txt'}
train
dev
test
Build Graph
	bertmodel.embeddings.word_embeddings.weight True
	bertmodel.embeddings.position_embeddings.weight True
	bertmodel.embeddings.token_type_embeddings.weight True
	bertmodel.embeddings.LayerNorm.weight True
	bertmodel.embeddings.LayerNorm.bias True
	bertmodel.encoder.layer.0.attention.self.query.weight False
	bertmodel.encoder.layer.0.attention.self.query.bias False
	bertmodel.encoder.layer.0.attention.self.key.weight False
	bertmodel.encoder.layer.0.attention.self.key.bias False
	bertmodel.encoder.layer.0.attention.self.value.weight False
	bertmodel.encoder.layer.0.attention.self.value.bias False
	bertmodel.encoder.layer.0.attention.output.dense.weight False
	bertmodel.encoder.layer.0.attention.output.dense.bias False
	bertmodel.encoder.layer.0.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.0.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.0.intermediate.dense.weight False
	bertmodel.encoder.layer.0.intermediate.dense.bias False
	bertmodel.encoder.layer.0.output.dense.weight False
	bertmodel.encoder.layer.0.output.dense.bias False
	bertmodel.encoder.layer.0.output.LayerNorm.weight False
	bertmodel.encoder.layer.0.output.LayerNorm.bias False
	bertmodel.encoder.layer.1.attention.self.query.weight False
	bertmodel.encoder.layer.1.attention.self.query.bias False
	bertmodel.encoder.layer.1.attention.self.key.weight False
	bertmodel.encoder.layer.1.attention.self.key.bias False
	bertmodel.encoder.layer.1.attention.self.value.weight False
	bertmodel.encoder.layer.1.attention.self.value.bias False
	bertmodel.encoder.layer.1.attention.output.dense.weight False
	bertmodel.encoder.layer.1.attention.output.dense.bias False
	bertmodel.encoder.layer.1.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.1.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.1.intermediate.dense.weight False
	bertmodel.encoder.layer.1.intermediate.dense.bias False
	bertmodel.encoder.layer.1.output.dense.weight False
	bertmodel.encoder.layer.1.output.dense.bias False
	bertmodel.encoder.layer.1.output.LayerNorm.weight False
	bertmodel.encoder.layer.1.output.LayerNorm.bias False
	bertmodel.encoder.layer.2.attention.self.query.weight False
	bertmodel.encoder.layer.2.attention.self.query.bias False
	bertmodel.encoder.layer.2.attention.self.key.weight False
	bertmodel.encoder.layer.2.attention.self.key.bias False
	bertmodel.encoder.layer.2.attention.self.value.weight False
	bertmodel.encoder.layer.2.attention.self.value.bias False
	bertmodel.encoder.layer.2.attention.output.dense.weight False
	bertmodel.encoder.layer.2.attention.output.dense.bias False
	bertmodel.encoder.layer.2.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.2.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.2.intermediate.dense.weight False
	bertmodel.encoder.layer.2.intermediate.dense.bias False
	bertmodel.encoder.layer.2.output.dense.weight False
	bertmodel.encoder.layer.2.output.dense.bias False
	bertmodel.encoder.layer.2.output.LayerNorm.weight False
	bertmodel.encoder.layer.2.output.LayerNorm.bias False
	bertmodel.encoder.layer.3.attention.self.query.weight False
	bertmodel.encoder.layer.3.attention.self.query.bias False
	bertmodel.encoder.layer.3.attention.self.key.weight False
	bertmodel.encoder.layer.3.attention.self.key.bias False
	bertmodel.encoder.layer.3.attention.self.value.weight False
	bertmodel.encoder.layer.3.attention.self.value.bias False
	bertmodel.encoder.layer.3.attention.output.dense.weight False
	bertmodel.encoder.layer.3.attention.output.dense.bias False
	bertmodel.encoder.layer.3.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.3.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.3.intermediate.dense.weight False
	bertmodel.encoder.layer.3.intermediate.dense.bias False
	bertmodel.encoder.layer.3.output.dense.weight False
	bertmodel.encoder.layer.3.output.dense.bias False
	bertmodel.encoder.layer.3.output.LayerNorm.weight False
	bertmodel.encoder.layer.3.output.LayerNorm.bias False
	bertmodel.encoder.layer.4.attention.self.query.weight False
	bertmodel.encoder.layer.4.attention.self.query.bias False
	bertmodel.encoder.layer.4.attention.self.key.weight False
	bertmodel.encoder.layer.4.attention.self.key.bias False
	bertmodel.encoder.layer.4.attention.self.value.weight False
	bertmodel.encoder.layer.4.attention.self.value.bias False
	bertmodel.encoder.layer.4.attention.output.dense.weight False
	bertmodel.encoder.layer.4.attention.output.dense.bias False
	bertmodel.encoder.layer.4.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.4.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.4.intermediate.dense.weight False
	bertmodel.encoder.layer.4.intermediate.dense.bias False
	bertmodel.encoder.layer.4.output.dense.weight False
	bertmodel.encoder.layer.4.output.dense.bias False
	bertmodel.encoder.layer.4.output.LayerNorm.weight False
	bertmodel.encoder.layer.4.output.LayerNorm.bias False
	bertmodel.encoder.layer.5.attention.self.query.weight False
	bertmodel.encoder.layer.5.attention.self.query.bias False
	bertmodel.encoder.layer.5.attention.self.key.weight False
	bertmodel.encoder.layer.5.attention.self.key.bias False
	bertmodel.encoder.layer.5.attention.self.value.weight False
	bertmodel.encoder.layer.5.attention.self.value.bias False
	bertmodel.encoder.layer.5.attention.output.dense.weight False
	bertmodel.encoder.layer.5.attention.output.dense.bias False
	bertmodel.encoder.layer.5.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.5.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.5.intermediate.dense.weight False
	bertmodel.encoder.layer.5.intermediate.dense.bias False
	bertmodel.encoder.layer.5.output.dense.weight False
	bertmodel.encoder.layer.5.output.dense.bias False
	bertmodel.encoder.layer.5.output.LayerNorm.weight False
	bertmodel.encoder.layer.5.output.LayerNorm.bias False
	bertmodel.encoder.layer.6.attention.self.query.weight False
	bertmodel.encoder.layer.6.attention.self.query.bias False
	bertmodel.encoder.layer.6.attention.self.key.weight False
	bertmodel.encoder.layer.6.attention.self.key.bias False
	bertmodel.encoder.layer.6.attention.self.value.weight False
	bertmodel.encoder.layer.6.attention.self.value.bias False
	bertmodel.encoder.layer.6.attention.output.dense.weight False
	bertmodel.encoder.layer.6.attention.output.dense.bias False
	bertmodel.encoder.layer.6.attention.output.LayerNorm.weight False
	bertmodel.encoder.layer.6.attention.output.LayerNorm.bias False
	bertmodel.encoder.layer.6.intermediate.dense.weight False
	bertmodel.encoder.layer.6.intermediate.dense.bias False
	bertmodel.encoder.layer.6.output.dense.weight False
	bertmodel.encoder.layer.6.output.dense.bias False
	bertmodel.encoder.layer.6.output.LayerNorm.weight False
	bertmodel.encoder.layer.6.output.LayerNorm.bias False
	bertmodel.encoder.layer.7.attention.self.query.weight False
	bertmodel.encoder.layer.7.attention.self.query.bias False
	bertmodel.encoder.layer.7.attention.self.key.weight False
	bertmodel.encoder.layer.7.attention.self.key.bias False
	bertmodel.encoder.layer.7.attention.self.value.weightTruncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
